## Updated on 2025.06.23
<details>
  <summary>Table of Contents</summary>
  <ol>
    <li><a href=#machine-unlearning>Machine Unlearning</a></li>
    <li><a href=#machine-unlearning-benchmarks>Machine Unlearning Benchmarks</a></li>
    <li><a href=#machine-unlearning-datasets>Machine Unlearning Datasets</a></li>
  </ol>
</details>

## Machine Unlearning

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-06-18**|**DAILOC: Domain-Incremental Learning for Indoor Localization using Smartphones**|Akhil Singampalli et.al.|[2506.15554](http://arxiv.org/abs/2506.15554)|null|
|**2025-06-18**|**An efficient forgetting-aware fine-tuning framework for pretrained universal machine-learning interatomic potentials**|Jisu Kim et.al.|[2506.15223](http://arxiv.org/abs/2506.15223)|null|
|**2025-06-18**|**Towards Reliable Forgetting: A Survey on Machine Unlearning Verification, Challenges, and Future Directions**|Lulu Xue et.al.|[2506.15115](http://arxiv.org/abs/2506.15115)|null|
|**2025-06-18**|**PDLRecover: Privacy-preserving Decentralized Model Recovery with Machine Unlearning**|Xiangman Li et.al.|[2506.15112](http://arxiv.org/abs/2506.15112)|null|
|**2025-06-04**|**Self-Composing Policies for Scalable Continual Reinforcement Learning**|Mikel Malagón et.al.|[2506.14811](http://arxiv.org/abs/2506.14811)|null|
|**2025-06-17**|**Don't Make It Up: Preserving Ignorance Awareness in LLM Fine-Tuning**|William F. Shen et.al.|[2506.14387](http://arxiv.org/abs/2506.14387)|null|
|**2025-06-16**|**Unlearning Isn't Invisible: Detecting Unlearning Traces in LLMs from Model Outputs**|Yiwei Chen et.al.|[2506.14003](http://arxiv.org/abs/2506.14003)|**[link](https://github.com/optml-group/unlearn-trace)**|
|**2025-06-16**|**Membership Inference Attacks as Privacy Tools: Reliability, Disparity and Ensemble**|Zhiqi Wang et.al.|[2506.13972](http://arxiv.org/abs/2506.13972)|**[link](https://github.com/RPI-DSPlab/mia-disparity)**|
|**2025-06-16**|**Sharpness-Aware Machine Unlearning**|Haoran Tang et.al.|[2506.13715](http://arxiv.org/abs/2506.13715)|null|
|**2025-06-16**|**Unlearning-Enhanced Website Fingerprinting Attack: Against Backdoor Poisoning in Anonymous Networks**|Yali Yuan et.al.|[2506.13563](http://arxiv.org/abs/2506.13563)|null|
|**2025-06-16**|**On Immutable Memory Systems for Artificial Agents: A Blockchain-Indexed Automata-Theoretic Framework Using ECDH-Keyed Merkle Chains**|Craig Steven Wright et.al.|[2506.13246](http://arxiv.org/abs/2506.13246)|null|
|**2025-06-16**|**Align-then-Unlearn: Embedding Alignment for LLM Unlearning**|Philipp Spohn et.al.|[2506.13181](http://arxiv.org/abs/2506.13181)|**[link](https://github.com/explainableml/align-then-unlearn)**|
|**2025-06-16**|**The Space Complexity of Learning-Unlearning Algorithms**|Yeshwanth Cherapanamjeri et.al.|[2506.13048](http://arxiv.org/abs/2506.13048)|null|
|**2025-06-19**|**A Comprehensive Survey on Continual Learning in Generative Models**|Haiyang Guo et.al.|[2506.13045](http://arxiv.org/abs/2506.13045)|**[link](https://github.com/ghy0501/awesome-continual-learning-in-generative-models)**|
|**2025-06-16**|**Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective**|Nima Naderloui et.al.|[2506.13009](http://arxiv.org/abs/2506.13009)|**[link](https://github.com/datasec-lab/ruli)**|
|**2025-06-15**|**Reasoning Model Unlearning: Forgetting Traces, Not Just Answers, While Preserving Reasoning Skills**|Changsheng Wang et.al.|[2506.12963](http://arxiv.org/abs/2506.12963)|null|
|**2025-06-14**|**OpenUnlearning: Accelerating LLM Unlearning via Unified Benchmarking of Methods and Metrics**|Vineeth Dorna et.al.|[2506.12618](http://arxiv.org/abs/2506.12618)|null|
|**2025-06-14**|**When Forgetting Triggers Backdoors: A Clean Unlearning Attack**|Marco Arazzi et.al.|[2506.12522](http://arxiv.org/abs/2506.12522)|null|
|**2025-06-14**|**Robust LLM Unlearning with MUDMAN: Meta-Unlearning with Disruption Masking And Normalization**|Filip Sondej et.al.|[2506.12484](http://arxiv.org/abs/2506.12484)|null|
|**2025-06-14**|**SPIRE: Conditional Personalization for Federated Diffusion Generative Models**|Kaan Ozkara et.al.|[2506.12303](http://arxiv.org/abs/2506.12303)|null|
|**2025-06-12**|**UCD: Unlearning in LLMs via Contrastive Decoding**|Vinith M. Suriyakumar et.al.|[2506.12097](http://arxiv.org/abs/2506.12097)|null|
|**2025-06-13**|**Machine Unlearning for Robust DNNs: Attribution-Guided Partitioning and Neuron Pruning in Noisy Environments**|Deliang Jin et.al.|[2506.11615](http://arxiv.org/abs/2506.11615)|null|
|**2025-06-12**|**Lifting Data-Tracing Machine Unlearning to Knowledge-Tracing for Foundation Models**|Yuwen Tan et.al.|[2506.11253](http://arxiv.org/abs/2506.11253)|null|
|**2025-06-06**|**You Only Fine-tune Once: Many-Shot In-Context Fine-Tuning for Large Language Model**|Wenchong He et.al.|[2506.11103](http://arxiv.org/abs/2506.11103)|null|
|**2025-06-12**|**GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models**|Evelyn Ma et.al.|[2506.10946](http://arxiv.org/abs/2506.10946)|null|
|**2025-06-13**|**Saturation Self-Organizing Map**|Igor Urbanik et.al.|[2506.10680](http://arxiv.org/abs/2506.10680)|**[link](https://github.com/radinyn/satsom)**|
|**2025-06-12**|**TreeLoRA: Efficient Continual Learning via Layer-Wise LoRAs Guided by a Hierarchical Gradient-Similarity Tree**|Yu-Yang Qian et.al.|[2506.10355](http://arxiv.org/abs/2506.10355)|**[link](https://github.com/zinyy/treelora)**|
|**2025-06-11**|**Prompt Attacks Reveal Superficial Knowledge Removal in Unlearning Methods**|Yeonwoo Jang et.al.|[2506.10236](http://arxiv.org/abs/2506.10236)|null|
|**2025-06-11**|**Apollo: A Posteriori Label-Only Membership Inference Attack Towards Machine Unlearning**|Liou Tang et.al.|[2506.09923](http://arxiv.org/abs/2506.09923)|**[link](https://github.com/lioutang/unlearn-apollo-attack)**|
|**2025-06-11**|**Reinforced Refinement with Self-Aware Expansion for End-to-End Autonomous Driving**|Haochen Liu et.al.|[2506.09800](http://arxiv.org/abs/2506.09800)|null|
|**2025-06-11**|**Auto-Compressing Networks**|Vaggelis Dorovatas et.al.|[2506.09714](http://arxiv.org/abs/2506.09714)|null|
|**2025-06-11**|**Analytic Task Scheduler: Recursive Least Squares Based Method for Continual Learning in Embodied Foundation Models**|Lipei Xie et.al.|[2506.09623](http://arxiv.org/abs/2506.09623)|null|
|**2025-06-11**|**Improved Supervised Fine-Tuning for Large Language Models to Mitigate Catastrophic Forgetting**|Fei Ding et.al.|[2506.09428](http://arxiv.org/abs/2506.09428)|null|
|**2025-06-11**|**ErrorEraser: Unlearning Data Bias for Improved Continual Learning**|Xuemei Cao et.al.|[2506.09347](http://arxiv.org/abs/2506.09347)|null|
|**2025-06-10**|**SoK: Machine Unlearning for Large Language Models**|Jie Ren et.al.|[2506.09227](http://arxiv.org/abs/2506.09227)|null|
|**2025-06-13**|**LLaVA-c: Continual Improved Visual Instruction Tuning**|Wenzhuo Liu et.al.|[2506.08666](http://arxiv.org/abs/2506.08666)|null|
|**2025-06-10**|**Hierarchical Neural Collapse Detection Transformer for Class Incremental Object Detection**|Duc Thanh Pham et.al.|[2506.08562](http://arxiv.org/abs/2506.08562)|null|
|**2025-06-09**|**SHIELD: Secure Hypernetworks for Incremental Expansion Learning Defense**|Patryk Krukowski et.al.|[2506.08255](http://arxiv.org/abs/2506.08255)|null|
|**2025-06-09**|**Dealing with the Evil Twins: Improving Random Augmentation by Addressing Catastrophic Forgetting of Diverse Augmentations**|Dongkyu Cho et.al.|[2506.08240](http://arxiv.org/abs/2506.08240)|null|
|**2025-06-09**|**BLUR: A Bi-Level Optimization Approach for LLM Unlearning**|Hadi Reisizadeh et.al.|[2506.08164](http://arxiv.org/abs/2506.08164)|**[link](https://github.com/optimai-lab/blurllmunlearning)**|
|**2025-06-09**|**LLM Unlearning Should Be Form-Independent**|Xiaotian Ye et.al.|[2506.07795](http://arxiv.org/abs/2506.07795)|null|
|**2025-06-09**|**Beyond Benchmarks: A Novel Framework for Domain-Specific LLM Evaluation and Knowledge Mapping**|Nitin Sharma et.al.|[2506.07658](http://arxiv.org/abs/2506.07658)|null|
|**2025-06-09**|**Leveraging Historical and Current Interests for Continual Sequential Recommendation**|Gyuseok Lee et.al.|[2506.07466](http://arxiv.org/abs/2506.07466)|null|
|**2025-06-09**|**DPFormer: Dynamic Prompt Transformer for Continual Learning**|Sheng-Kai Huang et.al.|[2506.07414](http://arxiv.org/abs/2506.07414)|null|
|**2025-06-09**|**An Intelligent Fault Self-Healing Mechanism for Cloud AI Systems via Integration of Large Language Models and Deep Reinforcement Learning**|Ze Yang et.al.|[2506.07411](http://arxiv.org/abs/2506.07411)|null|
|**2025-06-08**|**RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality**|Chenlong Zhang et.al.|[2506.07171](http://arxiv.org/abs/2506.07171)|null|
|**2025-06-10**|**Certified Unlearning for Neural Networks**|Anastasia Koloskova et.al.|[2506.06985](http://arxiv.org/abs/2506.06985)|**[link](https://github.com/stair-lab/certified-unlearning-neural-networks-icml-2025)**|
|**2025-06-07**|**Breaking Data Silos: Towards Open and Scalable Mobility Foundation Models via Generative Continual Learning**|Yuan Yuan et.al.|[2506.06694](http://arxiv.org/abs/2506.06694)|null|
|**2025-06-06**|**A Certified Unlearning Approach without Access to Source Data**|Umit Yigit Basaran et.al.|[2506.06486](http://arxiv.org/abs/2506.06486)|null|
|**2025-06-09**|**Distillation Robustifies Unlearning**|Bruce W. Lee et.al.|[2506.06278](http://arxiv.org/abs/2506.06278)|null|
|**2025-06-06**|**Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Unlearning Completeness**|Cheng-Long Wang et.al.|[2506.06112](http://arxiv.org/abs/2506.06112)|**[link](https://github.com/happy2git/unlearning_inference_iam)**|
|**2025-06-06**|**System-Aware Unlearning Algorithms: Use Lesser, Forget Faster**|Linda Lu et.al.|[2506.06073](http://arxiv.org/abs/2506.06073)|null|
|**2025-06-06**|**Hey, That's My Data! Label-Only Dataset Inference in Large Language Models**|Chen Xiong et.al.|[2506.06057](http://arxiv.org/abs/2506.06057)|null|
|**2025-06-06**|**Dynamic Mixture of Progressive Parameter-Efficient Expert Library for Lifelong Robot Learning**|Yuheng Lei et.al.|[2506.05985](http://arxiv.org/abs/2506.05985)|**[link](https://github.com/HarryLui98/DMPEL)**|
|**2025-06-06**|**Mitigating Catastrophic Forgetting with Adaptive Transformer Block Expansion in Federated Fine-Tuning**|Yujia Huo et.al.|[2506.05977](http://arxiv.org/abs/2506.05977)|null|
|**2025-06-06**|**Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness**|Rongzhe Wei et.al.|[2506.05735](http://arxiv.org/abs/2506.05735)|null|
|**2025-06-06**|**Being Strong Progressively! Enhancing Knowledge Distillation of Large Language Models through a Curriculum Learning Framework**|Lingyuan Liu et.al.|[2506.05695](http://arxiv.org/abs/2506.05695)|**[link](https://github.com/liuliuyuan6/POCL)**|
|**2025-06-06**|**Low-Resource Domain Adaptation for Speech LLMs via Text-Only Fine-Tuning**|Yangui Fang et.al.|[2506.05671](http://arxiv.org/abs/2506.05671)|null|
|**2025-06-05**|**Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models**|Taha Entesari et.al.|[2506.05314](http://arxiv.org/abs/2506.05314)|null|
|**2025-06-05**|**Quantifying Cross-Modality Memorization in Vision-Language Models**|Yuxin Wen et.al.|[2506.05198](http://arxiv.org/abs/2506.05198)|null|
|**2025-06-05**|**Hierarchical-Task-Aware Multi-modal Mixture of Incremental LoRA Experts for Embodied Continual Learning**|Ziqi Jia et.al.|[2506.04595](http://arxiv.org/abs/2506.04595)|null|
|**2025-06-04**|**A Generative Adaptive Replay Continual Learning Model for Temporal Knowledge Graph Reasoning**|Zhiyu Zhang et.al.|[2506.04083](http://arxiv.org/abs/2506.04083)|null|
|**2025-06-05**|**Adapt before Continual Learning**|Aojun Lu et.al.|[2506.03956](http://arxiv.org/abs/2506.03956)|**[link](https://github.com/byyx666/ACL_code)**|
|**2025-05-30**|**Continual Learning in Vision-Language Models via Aligned Model Merging**|Ghada Sokar et.al.|[2506.03189](http://arxiv.org/abs/2506.03189)|null|
|**2025-06-03**|**Targeted Forgetting of Image Subgroups in CLIP Models**|Zeliang Zhang et.al.|[2506.03117](http://arxiv.org/abs/2506.03117)|null|
|**2025-06-03**|**A Continual Offline Reinforcement Learning Benchmark for Navigation Tasks**|Anthony Kobanda et.al.|[2506.02883](http://arxiv.org/abs/2506.02883)|null|
|**2025-06-03**|**Learned Controllers for Agile Quadrotors in Pursuit-Evasion Games**|Alejandro Sanchez Roncero et.al.|[2506.02849](http://arxiv.org/abs/2506.02849)|null|
|**2025-06-06**|**Rethinking Machine Unlearning in Image Generation Models**|Renyang Liu et.al.|[2506.02761](http://arxiv.org/abs/2506.02761)|**[link](https://github.com/ryliu68/igmu)**|
|**2025-06-03**|**Small Aid, Big Leap: Efficient Test-Time Adaptation for Vision-Language Models with AdaptNet**|Xiao Chen et.al.|[2506.02671](http://arxiv.org/abs/2506.02671)|null|
|**2025-06-03**|**Rethinking Post-Unlearning Behavior of Large Vision-Language Models**|Minsung Kim et.al.|[2506.02541](http://arxiv.org/abs/2506.02541)|null|
|**2025-06-02**|**Towards Machine Unlearning for Paralinguistic Speech Processing**|Orchid Chetia Phukan et.al.|[2506.02230](http://arxiv.org/abs/2506.02230)|null|
|**2025-06-11**|**SALAD: Systematic Assessment of Machine Unlearing on LLM-Aided Hardware Design**|Zeng Wang et.al.|[2506.02089](http://arxiv.org/abs/2506.02089)|null|
|**2025-06-01**|**EWGN: Elastic Weight Generation and Context Switching in Deep Learning**|Shriraj P. Sawant et.al.|[2506.02065](http://arxiv.org/abs/2506.02065)|null|
|**2025-05-31**|**Enhancing Multimodal Continual Instruction Tuning with BranchLoRA**|Duzhen Zhang et.al.|[2506.02041](http://arxiv.org/abs/2506.02041)|null|
|**2025-06-02**|**DRAUN: An Algorithm-Agnostic Data Reconstruction Attack on Federated Unlearning Systems**|Hithem Lamri et.al.|[2506.01777](http://arxiv.org/abs/2506.01777)|null|
|**2025-06-02**|**Class Incremental Learning for Algorithm Selection**|Mate Botond Nemeth et.al.|[2506.01545](http://arxiv.org/abs/2506.01545)|null|
|**2025-06-02**|**ThinkEval: Practical Evaluation of Knowledge Preservation and Consistency in LLM Editing with Thought-based Knowledge Graphs**|Manit Baser et.al.|[2506.01386](http://arxiv.org/abs/2506.01386)|null|
|**2025-06-02**|**Invariance Makes LLM Unlearning Resilient Even to Unanticipated Downstream Fine-Tuning**|Changsheng Wang et.al.|[2506.01339](http://arxiv.org/abs/2506.01339)|**[link](https://github.com/optml-group/unlearn-ilu)**|
|**2025-06-03**|**Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack**|SeungBum Ha et.al.|[2506.01318](http://arxiv.org/abs/2506.01318)|null|
|**2025-06-01**|**Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning**|Yixin Wan et.al.|[2506.00876](http://arxiv.org/abs/2506.00876)|null|
|**2025-06-01**|**Speech Unlearning**|Jiali Cheng et.al.|[2506.00848](http://arxiv.org/abs/2506.00848)|null|
|**2025-06-01**|**Unlearning Inversion Attacks for Graph Neural Networks**|Jiahao Zhang et.al.|[2506.00808](http://arxiv.org/abs/2506.00808)|null|
|**2025-06-01**|**LIFT the Veil for the Truth: Principal Weights Emerge after Rank Reduction for Reasoning-Focused Supervised Fine-Tuning**|Zihang Liu et.al.|[2506.00772](http://arxiv.org/abs/2506.00772)|null|
|**2025-05-31**|**Existing Large Language Model Unlearning Evaluations Are Inconclusive**|Zhili Feng et.al.|[2506.00688](http://arxiv.org/abs/2506.00688)|null|
|**2025-05-31**|**iDPA: Instance Decoupled Prompt Attention for Incremental Medical Object Detection**|Huahui Yi et.al.|[2506.00406](http://arxiv.org/abs/2506.00406)|null|
|**2025-05-31**|**Keeping an Eye on LLM Unlearning: The Hidden Risk and Remedy**|Jie Ren et.al.|[2506.00359](http://arxiv.org/abs/2506.00359)|null|
|**2025-06-03**|**Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation**|Ahmed Elhady et.al.|[2506.00288](http://arxiv.org/abs/2506.00288)|null|
|**2025-06-08**|**Aligned but Blind: Alignment Increases Implicit Bias by Reducing Awareness of Race**|Lihao Sun et.al.|[2506.00253](http://arxiv.org/abs/2506.00253)|**[link](https://github.com/slhleosun/aligned-but-blind)**|
|**2025-05-30**|**Unlocking the Power of Rehearsal in Continual Learning: A Theoretical Perspective**|Junze Deng et.al.|[2506.00205](http://arxiv.org/abs/2506.00205)|null|
|**2025-05-29**|**Robot-R1: Reinforcement Learning for Enhanced Embodied Reasoning in Robotics**|Dongyoung Kim et.al.|[2506.00070](http://arxiv.org/abs/2506.00070)|null|
|**2025-05-30**|**When Large Multimodal Models Confront Evolving Knowledge:Challenges and Pathways**|Kailin Jiang et.al.|[2505.24449](http://arxiv.org/abs/2505.24449)|**[link](https://github.com/EVOKE-LMM/EVOKE)**|
|**2025-05-30**|**Model Unlearning via Sparse Autoencoder Subspace Guided Projections**|Xu Wang et.al.|[2505.24428](http://arxiv.org/abs/2505.24428)|null|
|**2025-05-30**|**Breaking the Gold Standard: Extracting Forgotten Data under Exact Unlearning in Large Language Models**|Xiaoyu Wu et.al.|[2505.24379](http://arxiv.org/abs/2505.24379)|null|
|**2025-05-30**|**Rethinking Continual Learning with Progressive Neural Collapse**|Zheng Wang et.al.|[2505.24254](http://arxiv.org/abs/2505.24254)|null|
|**2025-05-29**|**TRACE: Trajectory-Constrained Concept Erasure in Diffusion Models**|Finn Carter et.al.|[2505.23312](http://arxiv.org/abs/2505.23312)|null|
|**2025-05-29**|**LADA: Scalable Label-Specific CLIP Adapter for Continual Learning**|Mao-Lin Luo et.al.|[2505.23271](http://arxiv.org/abs/2505.23271)|**[link](https://github.com/maolinluo/lada)**|
|**2025-05-29**|**Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs**|Haokun Chen et.al.|[2505.23270](http://arxiv.org/abs/2505.23270)|null|
|**2025-06-05**|**MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking**|Yaxiong Lei et.al.|[2505.22769](http://arxiv.org/abs/2505.22769)|null|
|**2025-05-28**|**When Does Neuroevolution Outcompete Reinforcement Learning in Transfer Learning Tasks?**|Eleni Nisioti et.al.|[2505.22696](http://arxiv.org/abs/2505.22696)|**[link](https://github.com/eleninisioti/neuroevolution_in_transfer_learning)**|
|**2025-05-29**|**Pre-training for Recommendation Unlearning**|Guoxuan Chen et.al.|[2505.22649](http://arxiv.org/abs/2505.22649)|null|
|**2025-05-28**|**Machine Unlearning under Overparameterization**|Jacob L. Block et.al.|[2505.22601](http://arxiv.org/abs/2505.22601)|null|
|**2025-05-28**|**Precise In-Parameter Concept Erasure in Large Language Models**|Yoav Gur-Arieh et.al.|[2505.22586](http://arxiv.org/abs/2505.22586)|null|
|**2025-05-28**|**Less, but Better: Efficient Multilingual Expansion for LLMs via Layer-wise Mixture-of-Experts**|Xue Zhang et.al.|[2505.22582](http://arxiv.org/abs/2505.22582)|null|
|**2025-05-28**|**Frugal Incremental Generative Modeling using Variational Autoencoders**|Victor Enescu et.al.|[2505.22408](http://arxiv.org/abs/2505.22408)|null|
|**2025-06-16**|**Train with Perturbation, Infer after Merging: A Two-Stage Framework for Continual Learning**|Haomiao Qiu et.al.|[2505.22389](http://arxiv.org/abs/2505.22389)|null|
|**2025-05-28**|**Budget-Adaptive Adapter Tuning in Orthogonal Subspaces for Continual Learning in LLMs**|Zhiyi Wan et.al.|[2505.22358](http://arxiv.org/abs/2505.22358)|null|
|**2025-05-28**|**From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization**|Shoaib Ahmed Siddiqui et.al.|[2505.22310](http://arxiv.org/abs/2505.22310)|null|
|**2025-05-28**|**LoKI: Low-damage Knowledge Implanting of Large Language Models**|Runyu Wang et.al.|[2505.22120](http://arxiv.org/abs/2505.22120)|**[link](https://github.com/Nexround/LoKI)**|
|**2025-05-28**|**Targeted Unlearning Using Perturbed Sign Gradient Methods With Applications On Medical Images**|George R. Nahass et.al.|[2505.21872](http://arxiv.org/abs/2505.21872)|null|
|**2025-06-12**|**Measuring Representational Shifts in Continual Learning: A Linear Transformation Perspective**|Joonkyu Kim et.al.|[2505.20970](http://arxiv.org/abs/2505.20970)|null|
|**2025-05-27**|**Information-Theoretic Complementary Prompts for Improved Continual Text Classification**|Duzhen Zhang et.al.|[2505.20933](http://arxiv.org/abs/2505.20933)|null|
|**2025-05-27**|**Continual Learning on CLIP via Incremental Prompt Tuning with Intrinsic Textual Anchors**|Haodong Lu et.al.|[2505.20680](http://arxiv.org/abs/2505.20680)|null|
|**2025-05-27**|**Test-Time Learning for Large Language Models**|Jinwu Hu et.al.|[2505.20633](http://arxiv.org/abs/2505.20633)|null|
|**2025-05-26**|**Continuous Learning for Children's ASR: Overcoming Catastrophic Forgetting with Elastic Weight Consolidation and Synaptic Intelligence**|Edem Ahadzi et.al.|[2505.20216](http://arxiv.org/abs/2505.20216)|null|
|**2025-05-26**|**FunReason: Enhancing Large Language Models' Function Calling via Self-Refinement Multiscale Loss and Automated Data Refinement**|Bingguang Hao et.al.|[2505.20192](http://arxiv.org/abs/2505.20192)|**[link](https://github.com/bingguanghao/funreason)**|
|**2025-05-26**|**From Alignment to Advancement: Bootstrapping Audio-Language Alignment with Synthetic Data**|Chun-Yi Kuan et.al.|[2505.20166](http://arxiv.org/abs/2505.20166)|null|
|**2025-05-26**|**From Data to Modeling: Fully Open-vocabulary Scene Graph Generation**|Zuyao Chen et.al.|[2505.20106](http://arxiv.org/abs/2505.20106)|null|
|**2025-05-26**|**Data-Free Class-Incremental Gesture Recognition with Prototype-Guided Pseudo Feature Replay**|Hongsong Wang et.al.|[2505.20049](http://arxiv.org/abs/2505.20049)|**[link](https://github.com/sunao-101/pgpfr-3)**|
|**2025-05-26**|**Mixture of LoRA Experts for Low-Resourced Multi-Accent Automatic Speech Recognition**|Raphaël Bagat et.al.|[2505.20006](http://arxiv.org/abs/2505.20006)|null|
|**2025-05-26**|**Editing as Unlearning: Are Knowledge Editing Methods Strong Baselines for Large Language Model Unlearning?**|Zexi Li et.al.|[2505.19855](http://arxiv.org/abs/2505.19855)|null|
|**2025-05-26**|**Light distillation for Incremental Graph Convolution Collaborative Filtering**|X Fan et.al.|[2505.19810](http://arxiv.org/abs/2505.19810)|null|

<p align=right>(<a href=#updated-on-20250623>back to top</a>)</p>

## Machine Unlearning Benchmarks

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-06-18**|**Learning-Time Encoding Shapes Unlearning in LLMs**|Ruihan Wu et.al.|[2506.15076](http://arxiv.org/abs/2506.15076)|null|
|**2025-06-16**|**Rectifying Privacy and Efficacy Measurements in Machine Unlearning: A New Inference Attack Perspective**|Nima Naderloui et.al.|[2506.13009](http://arxiv.org/abs/2506.13009)|**[link](https://github.com/datasec-lab/ruli)**|
|**2025-06-12**|**UCD: Unlearning in LLMs via Contrastive Decoding**|Vinith M. Suriyakumar et.al.|[2506.12097](http://arxiv.org/abs/2506.12097)|null|
|**2025-06-12**|**GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models**|Evelyn Ma et.al.|[2506.10946](http://arxiv.org/abs/2506.10946)|null|
|**2025-06-06**|**Do LLMs Really Forget? Evaluating Unlearning with Knowledge Correlation and Confidence Awareness**|Rongzhe Wei et.al.|[2506.05735](http://arxiv.org/abs/2506.05735)|null|
|**2025-06-03**|**Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack**|SeungBum Ha et.al.|[2506.01318](http://arxiv.org/abs/2506.01318)|null|
|**2025-05-31**|**Existing Large Language Model Unlearning Evaluations Are Inconclusive**|Zhili Feng et.al.|[2506.00688](http://arxiv.org/abs/2506.00688)|null|
|**2025-05-29**|**Does Machine Unlearning Truly Remove Model Knowledge? A Framework for Auditing Unlearning in LLMs**|Haokun Chen et.al.|[2505.23270](http://arxiv.org/abs/2505.23270)|null|
|**2025-05-22**|**Harry Potter is Still Here! Probing Knowledge Leakage in Targeted Unlearned Large Language Models via Automated Adversarial Prompting**|Bang Trinh Tran To et.al.|[2505.17160](http://arxiv.org/abs/2505.17160)|null|
|**2025-05-26**|**"Alexa, can you forget me?" Machine Unlearning Benchmark in Spoken Language Understanding**|Alkis Koudounas et.al.|[2505.15700](http://arxiv.org/abs/2505.15700)|**[link](https://github.com/koudounasalkis/UnSLU-BENCH)**|
|**2025-05-27**|**SEPS: A Separability Measure for Robust Unlearning in LLMs**|Wonje Jeung et.al.|[2505.14832](http://arxiv.org/abs/2505.14832)|null|
|**2025-05-11**|**Efficient Machine Unlearning by Model Splitting and Core Sample Selection**|Maximilian Egger et.al.|[2505.07026](http://arxiv.org/abs/2505.07026)|null|
|**2025-05-08**|**WaterDrum: Watermarking for Data-centric Unlearning Metric**|Xinyang Lu et.al.|[2505.05064](http://arxiv.org/abs/2505.05064)|**[link](https://github.com/lululu008/waterdrum)**|
|**2025-05-01**|**Unlearning Sensitive Information in Multimodal LLMs: Benchmark and Attack-Defense Evaluation**|Vaidehi Patil et.al.|[2505.01456](http://arxiv.org/abs/2505.01456)|**[link](https://github.com/vaidehi99/unlok-vqa)**|
|**2025-06-13**|**AegisLLM: Scaling Agentic Systems for Self-Reflective Defense in LLM Security**|Zikui Cai et.al.|[2504.20965](http://arxiv.org/abs/2504.20965)|**[link](https://github.com/zikuicai/aegisllm)**|
|**2025-04-17**|**GRAIL: Gradient-Based Adaptive Unlearning for Privacy and Copyright in LLMs**|Kun-Woo Kim et.al.|[2504.12681](http://arxiv.org/abs/2504.12681)|null|
|**2025-04-16**|**LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks**|Soumyadeep Pal et.al.|[2504.10185](http://arxiv.org/abs/2504.10185)|**[link](https://github.com/optml-group/mu-coreset)**|
|**2025-03-24**|**Human Motion Unlearning**|Edoardo De Matteis et.al.|[2503.18674](http://arxiv.org/abs/2503.18674)|null|
|**2025-06-05**|**GRU: Mitigating the Trade-off between Unlearning and Retention for LLMs**|Yue Wang et.al.|[2503.09117](http://arxiv.org/abs/2503.09117)|null|
|**2025-05-16**|**Are We Truly Forgetting? A Critical Re-examination of Machine Unlearning Evaluation Protocols**|Yongwoo Kim et.al.|[2503.06991](http://arxiv.org/abs/2503.06991)|null|
|**2025-03-04**|**Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization**|Aviv Shamsian et.al.|[2503.02312](http://arxiv.org/abs/2503.02312)|null|
|**2025-02-20**|**Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models**|Haokun Chen et.al.|[2502.15836](http://arxiv.org/abs/2502.15836)|null|
|**2025-02-27**|**LUME: LLM Unlearning with Multitask Evaluations**|Anil Ramakrishna et.al.|[2502.15097](http://arxiv.org/abs/2502.15097)|null|
|**2025-02-19**|**Beyond Single-Value Metrics: Evaluating and Enhancing LLM Unlearning with Cognitive Diagnosis**|Yicheng Lang et.al.|[2502.13996](http://arxiv.org/abs/2502.13996)|null|
|**2025-02-25**|**SafeEraser: Enhancing Safety in Multimodal Large Language Models through Multimodal Machine Unlearning**|Junkai Chen et.al.|[2502.12520](http://arxiv.org/abs/2502.12520)|null|
|**2024-12-03**|**Improved Localized Machine Unlearning Through the Lens of Memorization**|Reihaneh Torkzadehmahani et.al.|[2412.02432](http://arxiv.org/abs/2412.02432)|null|
|**2025-02-23**|**Does Unlearning Truly Unlearn? A Black Box Evaluation of LLM Unlearning Methods**|Jai Doshi et.al.|[2411.12103](http://arxiv.org/abs/2411.12103)|**[link](https://github.com/jaidoshi/knowledge-erasure)**|
|**2025-03-07**|**Benchmarking Vision Language Model Unlearning via Fictitious Facial Identity Dataset**|Yingzi Ma et.al.|[2411.03554](http://arxiv.org/abs/2411.03554)|**[link](https://github.com/safolab-wisc/fiubench)**|
|**2025-05-26**|**RESTOR: Knowledge Recovery in Machine Unlearning**|Keivan Rezaei et.al.|[2411.00204](http://arxiv.org/abs/2411.00204)|**[link](https://github.com/k1rezaei/restor)**|
|**2025-02-14**|**Protecting Privacy in Multimodal Large Language Models with MLLMU-Bench**|Zheyuan Liu et.al.|[2410.22108](http://arxiv.org/abs/2410.22108)|**[link](https://github.com/franciscoliu/MLLMU-Bench)**|
|**2024-10-22**|**Exploring Forgetting in Large Language Model Pre-Training**|Chonghua Liao et.al.|[2410.17018](http://arxiv.org/abs/2410.17018)|null|
|**2025-03-21**|**Catastrophic Failure of LLM Unlearning via Quantization**|Zhiwei Zhang et.al.|[2410.16454](http://arxiv.org/abs/2410.16454)|**[link](https://github.com/zzwjames/failurellmunlearning)**|
|**2025-02-07**|**Do Unlearning Methods Remove Information from Language Model Weights?**|Aghyad Deeb et.al.|[2410.08827](http://arxiv.org/abs/2410.08827)|**[link](https://github.com/aghyad-deeb/unlearning_evaluation)**|
|**2025-03-09**|**Holistic Unlearning Benchmark: A Multi-Faceted Evaluation for Text-to-Image Diffusion Model Unlearning**|Saemi Moon et.al.|[2410.05664](http://arxiv.org/abs/2410.05664)|null|
|**2025-04-08**|**Position: LLM Unlearning Benchmarks are Weak Measures of Progress**|Pratiksha Thaker et.al.|[2410.02879](http://arxiv.org/abs/2410.02879)|null|
|**2024-10-02**|**Deep Unlearn: Benchmarking Machine Unlearning**|Xavier F. Cadet et.al.|[2410.01276](http://arxiv.org/abs/2410.01276)|null|
|**2024-09-18**|**MEOW: MEMOry Supervised LLM Unlearning Via Inverted Facts**|Tianle Gu et.al.|[2409.11844](http://arxiv.org/abs/2409.11844)|**[link](https://github.com/carol-gutianle/meow)**|
|**2024-12-22**|**CURE4Rec: A Benchmark for Recommendation Unlearning with Deeper Influence**|Chaochao Chen et.al.|[2408.14393](http://arxiv.org/abs/2408.14393)|**[link](https://github.com/xiye7lai/cure4rec)**|
|**2024-08-20**|**Towards Robust Knowledge Unlearning: An Adversarial Framework for Assessing and Improving Unlearning Robustness in Large Language Models**|Hongbang Yuan et.al.|[2408.10682](http://arxiv.org/abs/2408.10682)|null|
|**2024-08-12**|**TruVRF: Towards Triple-Granularity Verification on Machine Unlearning**|Chunyi Zhou et.al.|[2408.06063](http://arxiv.org/abs/2408.06063)|null|
|**2024-07-14**|**MUSE: Machine Unlearning Six-Way Evaluation for Language Models**|Weijia Shi et.al.|[2407.06460](http://arxiv.org/abs/2407.06460)|null|
|**2025-03-17**|**Unlearning or Obfuscating? Jogging the Memory of Unlearned LLMs via Benign Relearning**|Shengyuan Hu et.al.|[2406.13356](http://arxiv.org/abs/2406.13356)|**[link](https://github.com/s-huu/jog_llm_memory)**|
|**2024-10-04**|**Intrinsic Evaluation of Unlearning Using Parametric Knowledge Traces**|Yihuai Hong et.al.|[2406.11614](http://arxiv.org/abs/2406.11614)|**[link](https://github.com/yihuaihong/conceptvectors)**|
|**2024-06-16**|**RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models**|Zhuoran Jin et.al.|[2406.10890](http://arxiv.org/abs/2406.10890)|**[link](https://github.com/jinzhuoran/rwku)**|
|**2025-02-14**|**Towards Reliable Empirical Machine Unlearning Evaluation: A Cryptographic Game Perspective**|Yiwen Tu et.al.|[2404.11577](http://arxiv.org/abs/2404.11577)|null|
|**2024-04-01**|**Machine Unlearning for Traditional Models and Large Language Models: A Short Survey**|Yi Xu et.al.|[2404.01206](http://arxiv.org/abs/2404.01206)|null|
|**2024-04-30**|**Towards Lifecycle Unlearning Commitment Management: Measuring Sample-level Approximate Unlearning Completeness**|Cheng-Long Wang et.al.|[2403.12830](http://arxiv.org/abs/2403.12830)|null|
|**2024-04-01**|**Ethos: Rectifying Language Models in Orthogonal Parameter Space**|Lei Gao et.al.|[2403.08994](http://arxiv.org/abs/2403.08994)|null|
|**2024-02-26**|**Eight Methods to Evaluate Robust Unlearning in LLMs**|Aengus Lynch et.al.|[2402.16835](http://arxiv.org/abs/2402.16835)|null|
|**2024-10-29**|**UnlearnCanvas: Stylized Image Dataset for Enhanced Machine Unlearning Evaluation in Diffusion Models**|Yihua Zhang et.al.|[2402.11846](http://arxiv.org/abs/2402.11846)|**[link](https://github.com/optml-group/unlearncanvas)**|
|**2024-02-06**|**Parameter-tuning-free data entry error unlearning with adaptive selective synaptic dampening**|Stefan Schoepf et.al.|[2402.10098](http://arxiv.org/abs/2402.10098)|null|
|**2024-01-17**|**Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization**|Yoonhwa Jung et.al.|[2401.08998](http://arxiv.org/abs/2401.08998)|null|
|**2023-12-24**|**Towards Machine Unlearning Benchmarks: Forgetting the Personal Identities in Facial Recognition Systems**|Dasol Choi et.al.|[2311.02240](http://arxiv.org/abs/2311.02240)|**[link](https://github.com/ndb796/machineunlearning)**|
|**2023-10-15**|**Seeking Next Layer Neurons' Attention for Error-Backpropagation-Like Training in a Multi-Agent Network Framework**|Arshia Soltani Moakhar et.al.|[2310.09952](http://arxiv.org/abs/2310.09952)|null|
|**2023-05-11**|**KGA: A General Machine Unlearning Framework Based on Knowledge Gap Alignment**|Lingzhi Wang et.al.|[2305.06535](http://arxiv.org/abs/2305.06535)|**[link](https://github.com/lingzhi-wang/kgaunlearn)**|
|**2023-05-31**|**Deep Regression Unlearning**|Ayush K Tarun et.al.|[2210.08196](http://arxiv.org/abs/2210.08196)|**[link](https://github.com/ayu987/deep-regression-unlearning)**|
|**2023-05-31**|**Can Bad Teaching Induce Forgetting? Unlearning in Deep Networks using an Incompetent Teacher**|Vikram S Chundawat et.al.|[2205.08096](http://arxiv.org/abs/2205.08096)|**[link](https://github.com/vikram2000b/bad-teaching-unlearning)**|
|**2022-03-02**|**Unrolling SGD: Understanding Factors Influencing Machine Unlearning**|Anvith Thudi et.al.|[2109.13398](http://arxiv.org/abs/2109.13398)|**[link](https://github.com/cleverhans-lab/unrolling-sgd)**|
|**2020-09-14**|**CVPR 2020 Continual Learning in Computer Vision Competition: Approaches, Results, Current Challenges and Future Directions**|Vincenzo Lomonaco et.al.|[2009.09929](http://arxiv.org/abs/2009.09929)|null|
|**2021-03-02**|**Using Hindsight to Anchor Past Knowledge in Continual Learning**|Arslan Chaudhry et.al.|[2002.08165](http://arxiv.org/abs/2002.08165)|null|

<p align=right>(<a href=#updated-on-20250623>back to top</a>)</p>

## Machine Unlearning Datasets

|Publish Date|Title|Authors|PDF|Code|
|---|---|---|---|---|
|**2025-06-17**|**Train Once, Forget Precisely: Anchored Optimization for Efficient Post-Hoc Unlearning**|Prabhav Sanga et.al.|[2506.14515](http://arxiv.org/abs/2506.14515)|null|
|**2025-06-16**|**Sharpness-Aware Machine Unlearning**|Haoran Tang et.al.|[2506.13715](http://arxiv.org/abs/2506.13715)|null|
|**2025-06-14**|**When Forgetting Triggers Backdoors: A Clean Unlearning Attack**|Marco Arazzi et.al.|[2506.12522](http://arxiv.org/abs/2506.12522)|null|
|**2025-06-12**|**UCD: Unlearning in LLMs via Contrastive Decoding**|Vinith M. Suriyakumar et.al.|[2506.12097](http://arxiv.org/abs/2506.12097)|null|
|**2025-06-12**|**GUARD: Guided Unlearning and Retention via Data Attribution for Large Language Models**|Evelyn Ma et.al.|[2506.10946](http://arxiv.org/abs/2506.10946)|null|
|**2025-06-08**|**Hallucination at a Glance: Controlled Visual Edits and Fine-Grained Multimodal Learning**|Tianyi Bai et.al.|[2506.07227](http://arxiv.org/abs/2506.07227)|null|
|**2025-06-08**|**RULE: Reinforcement UnLEarning Achieves Forget-Retain Pareto Optimality**|Chenlong Zhang et.al.|[2506.07171](http://arxiv.org/abs/2506.07171)|null|
|**2025-06-05**|**Constrained Entropic Unlearning: A Primal-Dual Framework for Large Language Models**|Taha Entesari et.al.|[2506.05314](http://arxiv.org/abs/2506.05314)|null|
|**2025-06-06**|**Rethinking Machine Unlearning in Image Generation Models**|Renyang Liu et.al.|[2506.02761](http://arxiv.org/abs/2506.02761)|**[link](https://github.com/ryliu68/igmu)**|
|**2025-06-03**|**Unlearning's Blind Spots: Over-Unlearning and Prototypical Relearning Attack**|SeungBum Ha et.al.|[2506.01318](http://arxiv.org/abs/2506.01318)|null|
|**2025-06-01**|**Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning**|Yixin Wan et.al.|[2506.00876](http://arxiv.org/abs/2506.00876)|null|
|**2025-05-30**|**Model Unlearning via Sparse Autoencoder Subspace Guided Projections**|Xu Wang et.al.|[2505.24428](http://arxiv.org/abs/2505.24428)|null|
|**2025-05-28**|**Machine Unlearning under Overparameterization**|Jacob L. Block et.al.|[2505.22601](http://arxiv.org/abs/2505.22601)|null|
|**2025-05-28**|**From Dormant to Deleted: Tamper-Resistant Unlearning Through Weight-Space Regularization**|Shoaib Ahmed Siddiqui et.al.|[2505.22310](http://arxiv.org/abs/2505.22310)|null|
|**2025-05-26**|**ImgEdit: A Unified Image Editing Dataset and Benchmark**|Yang Ye et.al.|[2505.20275](http://arxiv.org/abs/2505.20275)|**[link](https://github.com/pku-yuangroup/imgedit)**|
|**2025-05-26**|**MolEditRL: Structure-Preserving Molecular Editing via Discrete Diffusion and Reinforcement Learning**|Yuanxin Zhuang et.al.|[2505.20131](http://arxiv.org/abs/2505.20131)|null|
|**2025-05-25**|**Beyond Editing Pairs: Fine-Grained Instructional Image Editing via Multi-Scale Learnable Regions**|Chenrui Ma et.al.|[2505.19352](http://arxiv.org/abs/2505.19352)|null|
|**2025-05-23**|**DetailFusion: A Dual-branch Framework with Detail Enhancement for Composed Image Retrieval**|Yuxin Yang et.al.|[2505.17796](http://arxiv.org/abs/2505.17796)|null|
|**2025-05-31**|**DUSK: Do Not Unlearn Shared Knowledge**|Wonje Jeung et.al.|[2505.15209](http://arxiv.org/abs/2505.15209)|**[link](https://github.com/AI-ISL/DUSK)**|
|**2025-05-23**|**UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models**|Qizhou Chen et.al.|[2505.12345](http://arxiv.org/abs/2505.12345)|null|
|**2025-05-13**|**Mirror Mirror on the Wall, Have I Forgotten it All? A New Framework for Evaluating Machine Unlearning**|Brennon Brimhall et.al.|[2505.08138](http://arxiv.org/abs/2505.08138)|null|
|**2025-05-08**|**WaterDrum: Watermarking for Data-centric Unlearning Metric**|Xinyang Lu et.al.|[2505.05064](http://arxiv.org/abs/2505.05064)|**[link](https://github.com/lululu008/waterdrum)**|
|**2025-05-07**|**OBLIVIATE: Robust and Practical Machine Unlearning for Large Language Models**|Xiaoyu Xu et.al.|[2505.04416](http://arxiv.org/abs/2505.04416)|null|
|**2025-06-09**|**BalancEdit: Dynamically Balancing the Generality-Locality Trade-off in Multi-modal Model Editing**|Dongliang Guo et.al.|[2505.01343](http://arxiv.org/abs/2505.01343)|null|
|**2025-04-17**|**Image Editing with Diffusion Models: A Survey**|Jia Wang et.al.|[2504.13226](http://arxiv.org/abs/2504.13226)|null|
|**2025-04-17**|**$\texttt{Complex-Edit}$ : CoT-Like Instruction Generation for Complexity-Controllable Image Editing Benchmark**|Siwei Yang et.al.|[2504.13143](http://arxiv.org/abs/2504.13143)|null|
|**2025-04-17**|**SHA256 at SemEval-2025 Task 4: Selective Amnesia -- Constrained Unlearning for Large Language Models via Knowledge Isolation**|Saransh Agrawal et.al.|[2504.12996](http://arxiv.org/abs/2504.12996)|**[link](https://github.com/LAB-FLAIR/Constrained-Unlearning-for-LLM)**|
|**2025-04-15**|**Omni $^2$ : Unifying Omnidirectional Image Generation and Editing in an Omni Model**|Liu Yang et.al.|[2504.11379](http://arxiv.org/abs/2504.11379)|null|
|**2025-04-16**|**LLM Unlearning Reveals a Stronger-Than-Expected Coreset Effect in Current Benchmarks**|Soumyadeep Pal et.al.|[2504.10185](http://arxiv.org/abs/2504.10185)|**[link](https://github.com/optml-group/mu-coreset)**|
|**2025-05-01**|**Omni-Dish: Photorealistic and Faithful Image Generation and Editing for Arbitrary Chinese Dishes**|Huijie Liu et.al.|[2504.09948](http://arxiv.org/abs/2504.09948)|null|
|**2025-04-13**|**SPICE: A Synergistic, Precise, Iterative, and Customizable Image Editing Workflow**|Kenan Tang et.al.|[2504.09697](http://arxiv.org/abs/2504.09697)|**[link](https://github.com/kenantang/spice)**|
|**2025-06-01**|**ADS-Edit: A Multimodal Knowledge Editing Dataset for Autonomous Driving Systems**|Chenxi Wang et.al.|[2503.20756](http://arxiv.org/abs/2503.20756)|**[link](https://github.com/zjunlp/easyedit)**|
|**2025-03-26**|**InsViE-1M: Effective Instruction-based Video Editing with Elaborate Dataset Construction**|Yuhui Wu et.al.|[2503.20287](http://arxiv.org/abs/2503.20287)|**[link](https://github.com/langmanbusi/insvie)**|
|**2025-03-20**|**SAUCE: Selective Concept Unlearning in Vision-Language Models with Sparse Autoencoders**|Qing Li et.al.|[2503.14530](http://arxiv.org/abs/2503.14530)|null|
|**2025-03-12**|**Group-robust Machine Unlearning**|Thomas De Min et.al.|[2503.09330](http://arxiv.org/abs/2503.09330)|**[link](https://github.com/tdemin16/group-robust_machine_unlearning)**|
|**2025-03-04**|**MindBridge: Scalable and Cross-Model Knowledge Editing via Memory-Augmented Modality**|Shuaike Li et.al.|[2503.02701](http://arxiv.org/abs/2503.02701)|**[link](https://github.com/crashbugger/mindbridge)**|
|**2025-03-04**|**Go Beyond Your Means: Unlearning with Per-Sample Gradient Orthogonalization**|Aviv Shamsian et.al.|[2503.02312](http://arxiv.org/abs/2503.02312)|null|
|**2025-02-20**|**UPCORE: Utility-Preserving Coreset Selection for Balanced Unlearning**|Vaidehi Patil et.al.|[2502.15082](http://arxiv.org/abs/2502.15082)|**[link](https://github.com/vaidehi99/upcore)**|
|**2025-05-28**|**Which Retain Set Matters for LLM Unlearning? A Case Study on Entity Unlearning**|Hwan Chang et.al.|[2502.11441](http://arxiv.org/abs/2502.11441)|null|
|**2025-03-12**|**Señorita-2M: A High-Quality Instruction-based Dataset for General Video Editing by Video Specialists**|Bojia Zi et.al.|[2502.06734](http://arxiv.org/abs/2502.06734)|null|
|**2025-04-29**|**REALEDIT: Reddit Edits As a Large-scale Empirical Dataset for Image Transformations**|Peter Sushko et.al.|[2502.03629](http://arxiv.org/abs/2502.03629)|null|
|**2025-02-05**|**TruePose: Human-Parsing-guided Attention Diffusion for Full-ID Preserving Pose Transfer**|Zhihong Xu et.al.|[2502.03426](http://arxiv.org/abs/2502.03426)|null|
|**2024-12-30**|**Knowledge Editing for Large Language Model with Knowledge Neuronal Ensemble**|Yongchang Li et.al.|[2412.20637](http://arxiv.org/abs/2412.20637)|null|
|**2024-12-23**|**Assessing Human Editing Effort on LLM-Generated Texts via Compression-Based Edit Distance**|Nicolas Devatine et.al.|[2412.17321](http://arxiv.org/abs/2412.17321)|null|
|**2025-02-12**|**The Utility and Complexity of in- and out-of-Distribution Machine Unlearning**|Youssef Allouah et.al.|[2412.09119](http://arxiv.org/abs/2412.09119)|null|
|**2025-05-06**|**HumanEdit: A High-Quality Human-Rewarded Dataset for Instruction-based Image Editing**|Jinbin Bai et.al.|[2412.04280](http://arxiv.org/abs/2412.04280)|**[link](https://github.com/viiika/humanedit)**|
|**2024-12-03**|**Improved Localized Machine Unlearning Through the Lens of Memorization**|Reihaneh Torkzadehmahani et.al.|[2412.02432](http://arxiv.org/abs/2412.02432)|null|
|**2024-12-01**|**Learning to Forget using Hypernetworks**|Jose Miguel Lara Rangel et.al.|[2412.00761](http://arxiv.org/abs/2412.00761)|null|
|**2025-03-29**|**AnyEdit: Mastering Unified High-Quality Image Editing for Any Idea**|Qifan Yu et.al.|[2411.15738](http://arxiv.org/abs/2411.15738)|null|
|**2024-11-22**|**VIVID-10M: A Dataset and Baseline for Versatile and Interactive Video Local Editing**|Jiahao Hu et.al.|[2411.15260](http://arxiv.org/abs/2411.15260)|null|
|**2024-11-21**|**GalaxyEdit: Large-Scale Image Editing Dataset with Enhanced Diffusion Adapter**|Aniruddha Bala et.al.|[2411.13794](http://arxiv.org/abs/2411.13794)|null|
|**2024-11-13**|**Neural Corrective Machine Unranking**|Jingrui Hou et.al.|[2411.08562](http://arxiv.org/abs/2411.08562)|null|
|**2024-11-16**|**Towards Operationalizing Right to Data Protection**|Abhinav Java et.al.|[2411.08506](http://arxiv.org/abs/2411.08506)|null|
|**2024-11-13**|**Machine Unlearning on Pre-trained Models by Residual Feature Alignment Using LoRA**|Laiqiao Qin et.al.|[2411.08443](http://arxiv.org/abs/2411.08443)|null|
|**2024-11-08**|**The Empirical Impact of Data Sanitization on Language Models**|Anwesan Pal et.al.|[2411.05978](http://arxiv.org/abs/2411.05978)|null|
|**2025-05-07**|**Learning from Convolution-based Unlearnable Datasets**|Dohyun Kim et.al.|[2411.01742](http://arxiv.org/abs/2411.01742)|**[link](https://github.com/aseriesof-tubes/RSK)**|
|**2025-06-05**|**CLIPErase: Efficient Unlearning of Visual-Textual Associations in CLIP**|Tianyu Yang et.al.|[2410.23330](http://arxiv.org/abs/2410.23330)|null|
|**2024-11-11**|**Attribute-to-Delete: Machine Unlearning via Datamodel Matching**|Kristian Georgiev et.al.|[2410.23232](http://arxiv.org/abs/2410.23232)|null|
|**2024-10-08**|**NegMerge: Consensual Weight Negation for Strong Machine Unlearning**|Hyoseo Kim et.al.|[2410.05583](http://arxiv.org/abs/2410.05583)|**[link](https://github.com/naver-ai/negmerge)**|
|**2024-10-26**|**Pioneering Reliable Assessment in Text-to-Image Knowledge Editing: Leveraging a Fine-Grained Dataset and an Innovative Criterion**|Hengrui Gu et.al.|[2409.17928](http://arxiv.org/abs/2409.17928)|**[link](https://github.com/hengrui-gu/t2iknowledgeediting)**|
|**2024-12-17**|**Alternate Preference Optimization for Unlearning Factual Knowledge in Large Language Models**|Anmol Mekala et.al.|[2409.13474](http://arxiv.org/abs/2409.13474)|**[link](https://github.com/molereddy/Alternate-Preference-Optimization)**|
|**2024-09-19**|**LLM Surgery: Efficient Knowledge Unlearning and Editing in Large Language Models**|Akshaj Kumar Veldanda et.al.|[2409.13054](http://arxiv.org/abs/2409.13054)|null|
|**2025-06-01**|**CKnowEdit: A New Chinese Knowledge Editing Dataset for Linguistics, Facts, and Logic Error Correction in LLMs**|Jizhan Fang et.al.|[2409.05806](http://arxiv.org/abs/2409.05806)|**[link](https://github.com/zjunlp/easyedit)**|
|**2024-08-30**|**Vote&Mix: Plug-and-Play Token Reduction for Efficient Vision Transformer**|Shuai Peng et.al.|[2408.17062](http://arxiv.org/abs/2408.17062)|null|
|**2024-12-21**|**Revisiting Machine Unlearning with Dimensional Alignment**|Seonguk Seo et.al.|[2407.17710](http://arxiv.org/abs/2407.17710)|null|
|**2024-09-16**|**Learning to Refuse: Towards Mitigating Privacy Risks in LLMs**|Zhenhua Liu et.al.|[2407.10058](http://arxiv.org/abs/2407.10058)|**[link](https://github.com/zhliu0106/learning-to-refuse)**|
|**2024-07-10**|**Machine Unlearning for Medical Imaging**|Reza Nasirigerdeh et.al.|[2407.07539](http://arxiv.org/abs/2407.07539)|null|
|**2024-12-11**|**Zero-Shot Class Unlearning in CLIP with Synthetic Samples**|A. Kravets et.al.|[2407.07485](http://arxiv.org/abs/2407.07485)|**[link](https://github.com/akres001/zero-shot-class-unlearning-in-clip-with-synthetic-samples)**|
|**2024-12-19**|**UltraEdit: Instruction-based Fine-Grained Image Editing at Scale**|Haozhe Zhao et.al.|[2407.05282](http://arxiv.org/abs/2407.05282)|**[link](https://github.com/pkunlp-icler/ultraedit)**|
|**2024-08-27**|**BayTTA: Uncertainty-aware medical image classification with optimized test-time augmentation using Bayesian model averaging**|Zeinab Sherkatghanad et.al.|[2406.17640](http://arxiv.org/abs/2406.17640)|**[link](https://github.com/z-sherkat/baytta)**|
|**2025-05-19**|**Unveiling Entity-Level Unlearning for Large Language Models: A Comprehensive Analysis**|Weitao Ma et.al.|[2406.15796](http://arxiv.org/abs/2406.15796)|null|
|**2025-06-06**|**Opt-Out: Investigating Entity-Level Unlearning for Large Language Models via Optimal Transport**|Minseok Choi et.al.|[2406.12329](http://arxiv.org/abs/2406.12329)|**[link](https://github.com/brightjade/Opt-Out)**|
|**2024-06-16**|**RWKU: Benchmarking Real-World Knowledge Unlearning for Large Language Models**|Zhuoran Jin et.al.|[2406.10890](http://arxiv.org/abs/2406.10890)|**[link](https://github.com/jinzhuoran/rwku)**|
|**2024-09-11**|**Potion: Towards Poison Unlearning**|Stefan Schoepf et.al.|[2406.09173](http://arxiv.org/abs/2406.09173)|**[link](https://github.com/if-loops/towards_poison_unlearning)**|
|**2024-06-05**|**Nonlinear Transformations Against Unlearnable Datasets**|Thushari Hapuarachchi et.al.|[2406.02883](http://arxiv.org/abs/2406.02883)|null|
|**2024-10-30**|**What makes unlearning hard and what to do about it**|Kairan Zhao et.al.|[2406.01257](http://arxiv.org/abs/2406.01257)|**[link](https://github.com/kairanzhao/rum)**|
|**2024-10-13**|**ModelLock: Locking Your Model With a Spell**|Yifeng Gao et.al.|[2405.16285](http://arxiv.org/abs/2405.16285)|null|
|**2025-02-25**|**Everything is Editable: Extend Knowledge Editing to Unstructured Data in Large Language Models**|Jingcheng Deng et.al.|[2405.15349](http://arxiv.org/abs/2405.15349)|**[link](https://github.com/TrustedLLM/UnKE)**|
|**2024-12-17**|**Multi-Modal Recommendation Unlearning for Legal, Licensing, and Modality Constraints**|Yash Sinha et.al.|[2405.15328](http://arxiv.org/abs/2405.15328)|null|
|**2024-10-09**|**Defensive Unlearning with Adversarial Training for Robust Concept Erasure in Diffusion Models**|Yimeng Zhang et.al.|[2405.15234](http://arxiv.org/abs/2405.15234)|**[link](https://github.com/optml-group/advunlearn)**|
|**2024-05-23**|**EditWorld: Simulating World Dynamics for Instruction-Following Image Editing**|Ling Yang et.al.|[2405.14785](http://arxiv.org/abs/2405.14785)|**[link](https://github.com/yangling0818/editworld)**|
|**2024-05-31**|**ReasonPix2Pix: Instruction Reasoning Dataset for Advanced Image Editing**|Ying Jin et.al.|[2405.11190](http://arxiv.org/abs/2405.11190)|null|
|**2024-11-15**|**Provably Unlearnable Data Examples**|Derui Wang et.al.|[2405.03316](http://arxiv.org/abs/2405.03316)|**[link](https://github.com/neuralsec/certified-data-learnability)**|
|**2025-03-14**|**Lifelong Knowledge Editing for LLMs with Retrieval-Augmented Continuous Prompt Learning**|Qizhou Chen et.al.|[2405.03279](http://arxiv.org/abs/2405.03279)|**[link](https://github.com/qizhou000/RECIPE)**|
|**2024-05-06**|**To Each (Textual Sequence) Its Own: Improving Memorized-Data Unlearning in Large Language Models**|George-Octavian Barbulescu et.al.|[2405.03097](http://arxiv.org/abs/2405.03097)|null|
|**2025-03-20**|**Paint by Inpaint: Learning to Add Image Objects by Removing Them First**|Navve Wasserman et.al.|[2404.18212](http://arxiv.org/abs/2404.18212)|**[link](https://github.com/RotsteinNoam/Paint-by-Inpaint)**|
|**2024-04-19**|**Is Retain Set All You Need in Machine Unlearning? Restoring Performance of Unlearned Models with Out-Of-Distribution Images**|Jacopo Bonato et.al.|[2404.12922](http://arxiv.org/abs/2404.12922)|**[link](https://github.com/jbonato1/scar)**|
|**2024-12-15**|**StyleBooth: Image Style Editing with Multimodal Instruction**|Zhen Han et.al.|[2404.12154](http://arxiv.org/abs/2404.12154)|**[link](https://github.com/modelscope/scepter)**|
|**2024-04-15**|**HQ-Edit: A High-Quality Dataset for Instruction-based Image Editing**|Mude Hui et.al.|[2404.09990](http://arxiv.org/abs/2404.09990)|null|
|**2024-06-05**|**EffiVED:Efficient Video Editing via Text-instruction Diffusion Models**|Zhenghao Zhang et.al.|[2403.11568](http://arxiv.org/abs/2403.11568)|**[link](https://github.com/alibaba/effived)**|
|**2024-05-27**|**Make Me Happier: Evoking Emotions Through Image Diffusion Models**|Qing Lin et.al.|[2403.08255](http://arxiv.org/abs/2403.08255)|null|
|**2024-07-09**|**Challenging Forgets: Unveiling the Worst-Case Forget Sets in Machine Unlearning**|Chongyu Fan et.al.|[2403.07362](http://arxiv.org/abs/2403.07362)|**[link](https://github.com/optml-group/unlearn-worstcase)**|
|**2024-04-17**|**mEdIT: Multilingual Text Editing via Instruction Tuning**|Vipul Raheja et.al.|[2402.16472](http://arxiv.org/abs/2402.16472)|**[link](https://github.com/vipulraheja/medit)**|
|**2024-02-13**|**Discriminative Adversarial Unlearning**|Rohan Sharma et.al.|[2402.06864](http://arxiv.org/abs/2402.06864)|null|
|**2024-01-31**|**Propagation and Pitfalls: Reasoning-based Assessment of Knowledge Editing through Counterfactual Tasks**|Wenyue Hua et.al.|[2401.17585](http://arxiv.org/abs/2401.17585)|null|
|**2024-01-17**|**Attack and Reset for Unlearning: Exploiting Adversarial Noise toward Machine Unlearning through Parameter Re-initialization**|Yoonhwa Jung et.al.|[2401.08998](http://arxiv.org/abs/2401.08998)|null|
|**2024-01-11**|**TOFU: A Task of Fictitious Unlearning for LLMs**|Pratyush Maini et.al.|[2401.06121](http://arxiv.org/abs/2401.06121)|null|
|**2023-12-22**|**Fast-NTK: Parameter-Efficient Unlearning for Large-Scale Models**|Guihong Li et.al.|[2312.14923](http://arxiv.org/abs/2312.14923)|null|
|**2023-12-20**|**Retrieval-augmented Multilingual Knowledge Editing**|Weixuan Wang et.al.|[2312.13040](http://arxiv.org/abs/2312.13040)|**[link](https://github.com/vicky-wil/remake)**|
|**2023-12-20**|**CoIE: Chain-of-Instruct Editing for Multi-Attribute Face Manipulation**|Zhenduo Zhang et.al.|[2312.07879](http://arxiv.org/abs/2312.07879)|null|
|**2023-11-29**|**Rethinking Image Editing Detection in the Era of Generative AI Revolution**|Zhihao Sun et.al.|[2311.17953](http://arxiv.org/abs/2311.17953)|null|
|**2024-10-10**|**Unlearning via Sparse Representations**|Vedant Shah et.al.|[2311.15268](http://arxiv.org/abs/2311.15268)|null|
|**2023-11-21**|**CovarNav: Machine Unlearning via Model Inversion and Covariance Navigation**|Ali Abbasi et.al.|[2311.12999](http://arxiv.org/abs/2311.12999)|null|
|**2023-10-16**|**Enhancing Interpretability using Human Similarity Judgements to Prune Word Embeddings**|Natalia Flechas Manrique et.al.|[2310.10262](http://arxiv.org/abs/2310.10262)|null|

<p align=right>(<a href=#updated-on-20250623>back to top</a>)</p>

